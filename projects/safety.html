<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ML + Safety</title>

  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">

  <!-- Override Primary Color -->
  <style>
    /* Override background and text color for primary utilities */
    .bg-primary {
        background-color: #57068C !important;
    }

    .text-primary {
        color: #57068C !important;
    }

    .btn-primary {
        background-color: #57068C !important;
        border-color: #57068C !important;
    }

    .btn-primary:hover {
        background-color: #5a32a3 !important;
        border-color: #5a32a3 !important;
    }

    .border-primary {
        border-color: #57068C !important;
    }
  </style>
</head>
<body>

  <!-- Project Display -->
  <header class="bg-primary text-white py-4">
    <div class="container d-flex justify-content-between align-items-center">
      <h1 id="display-title" class="h2 mb-0">Machine Learning + Safety</h1>
      <div><a href="/index.html" class="text-white">Open Networks and Big Data Lab</a></div>
    </div>

  </header>

  <main class="container my-5">
    <section class="mb-5">
      <h2 class="h3 border-bottom pb-2 text-primary">Problem Motivation</h2>
      <p id="display-motivation">
        Modern intelligent systems operate where <strong>context, interpretation, and alignment</strong> determine their utility and trustworthiness.
        From enforcing privacy norms in enterprise communication, to aligning recommender behavior with expert rules, to tailoring policies for diverse
        sub-populations, and finally to probing how large language models absorb rules purely in context, our work asks a common question:
        <em>how can systems internalize and act on information responsibly?</em>
      </p>
      <p>
        We began by rethinking enterprise privacy from the vantage of <em>contextual integrity</em>, separating <em>what</em> is shared from <em>why</em> it is shared.
        We then introduced <em>domain-specific concordance</em> to ensure neural recommenders obey expert-defined categorical rules—bridging data-driven learning with domain knowledge.
        For development policy, we created <em>outcome-aware clustering</em> to derive targeted interventions that respect population heterogeneity.
        At the algorithmic core, <em>Contra</em> advanced controlled variable selection with stronger false-discovery control under misspecification.
        Finally, we studied <em>in-context alignment at scale</em>, revealing how LLMs degrade as in-prompt behaviors proliferate and highlighting robust evaluation and prompt design needs.
      </p>
    </section>
    
    <section class="mb-5">
      <h2 class="h3 border-bottom pb-2 text-primary">Results and Contributions</h2>
      <!-- A. Pollution -->
      <ol>
        <li><h4 class="h6 pb-2 text-primary">Contextual Integrity for Privacy Enforcement
        <small class="text-muted d-block">Paper: <em>VACCINE: Using Contextual Integrity for Data Leakage Detection</em> (WWW 2019)</small></h4>
        <ul>
            <li>CI-based formalism decoupling <em>flow extraction</em> from <em>policy enforcement</em> in DLP systems.</li>
            <li><strong>VACCINE</strong>: NLP-driven flow extraction + declarative policies compiled to operational SQL rules.</li>
            <li>Supports temporal/consent-aware rules; improved precision and expressivity on Enron email corpus.</li>
        </ul>
        </li><br>
        <li><h4 class="h6 pb-2 text-primary">Domain-Specific Concordance in Recommender Systems
        <small class="text-muted d-block">Paper: <em>Enhancing Neural Recommender Models through Domain-Specific Concordance</em> (WSDM 2021)</small></h4>
        <ul>
            <li>Regularization framework to make recommenders obey expert-defined <em>category mappings</em> under within-category perturbations.</li>
            <li>Improved category-robustness distance (≈ 101–126%) and accuracy (up to ≈ 12%) on MovieLens, Last.fm, and MIMIC-III.</li>
            <li>Bridges embeddings with human-understandable rules for robust generalization.</li>
       </ul>
        </li><br>
        <li>
        <h4 class="h6 pb-2 text-primary">Outcome-Aware Clustering for Targeted Policy Design
        <small class="text-muted d-block">Paper: <em>Targeted Policy Recommendations using Outcome-aware Clustering</em> (ACM COMPASS 2022)</small></h4>
        <ul>
             <li>Partially supervised segmentation that ties feature selection and distance to an <em>outcome</em> of interest.</li>
             <li>Balanced, near-homogeneous clusters enable <em>cluster-level</em> policy recommendations rather than one-size-fits-all.</li>
             <li>Applied to LSMS-ISA data for sub-Saharan Africa; surfaced policy heterogeneity invisible at population level.</li>
        </ul>
        </li><br>
        <li><h4 class="h6 pb-2 text-primary">Contrarian Randomization Tests (Contra) for Controlled Discovery
        <small class="text-muted d-block">Paper: <em>Contra: Contrarian Statistics for Controlled Variable Selection</em> (AISTATS 2021)</small></h4>
        <ul>
           <li>Mixture of two “contrarian” models (true vs. null-swapped) yields stronger FDR control when covariate models are misspecified.</li>
           <li>Maintains asymptotic power 1; more reliable p-values than calibrated HRTs; scalable to high dimensions/large n.</li>
           <li>Demonstrated on synthetic and genetic datasets with improved rigor and efficiency.</li>
        </ul>
        </li><br>
        <li><h4 class="h6 pb-2 text-primary">In-Context Alignment at Scale (When More is Less)
        <small class="text-muted d-block">Paper: <em>In-Context Alignment at Scale: When More is Less</em> (ICML 2025 Workshop)</small></h4>
        <ul>
           <li>Systematic study of LLMs’ ability to adopt novel rules/facts purely via prompts as the number of behaviors increases.</li>
           <li>Findings: accuracy degrades (often up to ~50%) with more in-context rules; depth/position effects; emergence of “cheating” via superficial cues.</li>
           <li>Introduces synthetic rule-following setups and <em>NewNews</em> to evaluate belief updating and instruction fidelity.</li>
        </ul>
      </li>
    </ol>
    </section>

    <section class="mb-5">
      <h2 class="h3 border-bottom pb-2 text-primary">Members</h2>
      <ol id="display-team" class="list-styled">
        <li>Neelabh Madan, NYU.</li>
        <li>Ananth Balashankar, NYU.</li>
        <li>Mukund Sudarshan, NYU.</li>

        <!-- <li>Lakshmi Subramanian, NYU.</li> -->
      </ol>
    </section>

    <section class="mb-5">
      <h2 class="h3 border-bottom pb-2 text-primary">Publications</h2>
      <ol id="display-publications" class="list-styled">
        <li class="mb-2">Yan Shvartzshnaider, Zvonimir Pavlinovic, Ananth Balashankar, Thomas Wies, Lakshminarayanan Subramanian, Helen Nissenbaum, Prateek Mittal. 
            <i>"VACCINE: Using Contextual Integrity for Data Leakage Detection."</i>
                World Wide Web Conference (WWW), 2019.  <a href="https://cs.nyu.edu/~wies/publ/vaccine.pdf">(PDF)</a>
        </li>
        <li class="mb-2">Ananth Balashankar, Alex Beutel, Lakshminarayanan Subramanian
            <i>"Enhancing Neural Recommender Models through Domain-Specific Concordance."</i>
                WSDM, 2021. <a href="https://dl.acm.org/doi/10.1145/3437963.3441784">(Link)</a>
        </li>
         <li class="mb-2">Ananth Balashankar, Samuel Fraiberger, Eric M. Deregt, Marelize Görgens, Lakshminarayanan Subramanian
            <i>"Targeted Policy Recommendations using Outcome-aware Clustering."</i>
                ACM COMPASS, 2022. <a href="https://dl.acm.org/doi/abs/10.1145/3530190.3534797">(Link)</a>
        </li>
         <li class="mb-2">Mukund Sudarshan, Aahlad Puli, Lakshminarayanan Subramanian, Sriram Sankararaman, Rajesh Ranganath.
            <i>"Contra: Contrarian Statistics for Controlled Variable Selection."</i>
                AISTATS, 2021. <a href="https://proceedings.mlr.press/v130/sudarshan21a.html">(Link)</a>
        </li>
        <li class="mb-2">Neelabh Madan, Lakshminarayanan Subramanian.
            <i>"In-Context Alignment at Scale: When More is Less."</i>
                ICML Workshop on Models of Human Feedback for AI Alignment, 2025. <a href="https://openreview.net/forum?id=IaL2J1eAbt">(Open Review)</a>
        </li>
      </ol>
    </section>
  </main>

  <!-- Footer -->
  <footer class="bg-light text-center text-muted py-4 mt-5 border-top">
    &copy; 2025 Open Networks & Big Data Lab | Courant Institute of Mathematical Sciences | Department of Computer Science, New York University.
  </footer>

</body>
</html>